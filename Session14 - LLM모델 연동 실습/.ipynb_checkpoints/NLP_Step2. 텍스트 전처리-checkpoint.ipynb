{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ff4a92-f1c9-4f34-aba0-baa2a46dcf3f",
   "metadata": {},
   "source": [
    "### 1. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1ad448-e257-4a83-8437-41f6400a91e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words:\n",
      "['Tokenization', 'is', 'the', 'initial', 'step', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'involving', 'the', 'division', 'of', 'a', 'larger', 'text', 'into', 'smaller', 'units', ',', 'which', 'are', 'known', 'as', 'tokens', '.', 'Tokens', 'serve', 'as', 'the', 'fundamental', 'building', 'blocks', 'for', 'processing', 'text', ',', 'where', 'each', 'token', 'can', 'represent', 'a', 'word', ',', 'phrase', ',', 'or', 'even', 'an', 'entire', 'sentence', 'within', 'the', 'context', 'of', 'a', 'larger', 'document', '.']\n",
      "\n",
      "\n",
      "Tokenized Sentences:\n",
      "['Tokenization is the initial step in Natural Language Processing (NLP), involving the division of a larger text into smaller units, which are known as tokens.', 'Tokens serve as the fundamental building blocks for processing text, where each token can represent a word, phrase, or even an entire sentence within the context of a larger document.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Tokenization is the initial step in Natural Language Processing (NLP), involving the division of a larger text into smaller units, which are known as tokens. Tokens serve as the fundamental building blocks for processing text, where each token can represent a word, phrase, or even an entire sentence within the context of a larger document.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Tokenized Words:\")\n",
    "print(words)\n",
    "\n",
    "# Print the tokenized sentences\n",
    "print(\"\\n\\nTokenized Sentences:\")\n",
    "print(sentences)\n",
    "\n",
    "## Output: Tokenized Words:\n",
    "# ['Tokenization', 'is', 'the', 'initial', 'step', 'in', 'Natural', 'Language', 'Processing', ',', 'involving', 'the', 'division', 'of', 'a', 'larger', 'text', 'into', 'smaller', 'units', ',', 'which', 'are', 'known', 'as', 'tokens', '.', 'Tokens', 'serve', 'as', 'the', 'fundamental', 'building', 'blocks', 'for', 'processing', 'text', ',', 'where', 'each', 'token', 'can', 'represent', 'a', 'word', ',', 'phrase', ',', 'or', 'even', 'an', 'entire', 'sentence', 'within', 'the', 'context', 'of', 'a', 'larger', 'document', '.']\n",
    "\n",
    "\n",
    "# Tokenized Sentences:\n",
    "# ['Tokenization is the initial step in Natural Language Processing, involving the division of a larger text into smaller units, which are known as tokens.', \n",
    "# 'Tokens serve as the fundamental building blocks for processing text, where each token can represent a word, phrase, or even an entire sentence within the context of a larger document.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424493f-2c1e-4483-8137-657662989ae5",
   "metadata": {},
   "source": [
    "### 2. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4beb59d-a6e4-46a8-abb1-a940993a1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'happili', 'better', 'jump', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer  \n",
    "\n",
    "# Specify the language for the stemmer (e.g., English) \n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)  \n",
    "\n",
    "## Output: ['run', 'fli', 'happili', 'better', 'jump', 'happiest']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f085c2d-37af-45a2-95ad-f7c5433664ef",
   "metadata": {},
   "source": [
    "### 3. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777553be-7a7b-40ea-ad00-d91ad648898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'fly', 'happily', 'better', 'jump', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)\n",
    "\n",
    "## Output: ['running', 'fly', 'happily', 'better', 'jump', 'happiest']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a2126-f269-4b68-93fc-d37455b3af2e",
   "metadata": {},
   "source": [
    "### 4. Part Of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a756730-f2e5-4530-aa21-babf99a04b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags\n",
      " [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')] \n",
      "\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "print(\"\")\n",
    "print(\"POS Tags\\n\", pos_tags, \"\\n\")\n",
    "\n",
    "nltk.download('tagsets')\n",
    "\n",
    "# Print the descriptions\n",
    "for (word, tag) in pos_tags:\n",
    "    print(nltk.help.upenn_tagset(tagpattern=tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9a413-6b54-481a-89a0-7c302c4469a5",
   "metadata": {},
   "source": [
    "### 5. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e84d9a-81ed-4b74-afe4-bccd3f51814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,328.0,168.0\" width=\"328px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"53.6585%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"22.7273%\" x=\"22.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.0909%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"31.8182%\" x=\"45.4545%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">brown</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"22.7273%\" x=\"77.2727%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.6364%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.8293%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"19.5122%\" x=\"53.6585%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barked</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.4146%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"19.5122%\" x=\"73.1707%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">loudly</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.9268%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.31707%\" x=\"92.6829%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.3415%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('brown', 'NN'), ('dog', 'NN')]), ('barked', 'VBD'), ('loudly', 'RB'), ('.', '.')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentence = \"The big brown dog barked loudly.\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "chunked = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "reg_exp = \"NP: {<DT>?<JJ>*<NN>*}\"\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result = rp.parse(chunked)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e4c6c-1495-4415-9b0a-66f90534c029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
