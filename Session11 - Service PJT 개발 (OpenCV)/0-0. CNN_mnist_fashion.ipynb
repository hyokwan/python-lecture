{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반 CPU 활용 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D, Conv2D, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test,y_test) =\\\n",
    "    keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 정답지를 해석할 수 있는 리스트\n",
    "## 예시 fashion_mnist_labels[9] = Ankleboot\n",
    "fashion_mnist_labels = [\"T-shrt/top\",\n",
    "                        \"Trouser\",\n",
    "                        \"Pullover\",\n",
    "                        \"Dress\",\n",
    "                        \"Coat\",\n",
    "                        \"Sandal\",\n",
    "                        \"Shirt\",\n",
    "                        \"Sneaker\",\n",
    "                        \"Bag\",\n",
    "                        \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_mnist_labels[ y_train[0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 분리 및 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_trainClean = x_train.astype(\"float32\") / 255\n",
    "x_testClean = x_test.astype(\"float32\") / 255\n",
    "\n",
    "totalTrainImgLen = x_trainClean.shape[0]\n",
    "totalTestImgLen = x_testClean.shape[0]\n",
    "\n",
    "imgSize = x_train[0].shape[0]\n",
    "\n",
    "w,h = imgSize, imgSize\n",
    "\n",
    "x_trainClean = x_trainClean.reshape( totalTrainImgLen, w, h, 1)\n",
    "x_testClean = x_testClean.reshape( totalTestImgLen, w, h, 1)\n",
    "\n",
    "y_trainOne = to_categorical(y_train, 10)\n",
    "y_testOne = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelInputDim = x_trainClean[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### train \n",
    "### features -> x_trainClean\n",
    "### label -> y_trainOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 0. inputLayer! (입력데이터 첫번째 shape 들어가야한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D( filters=32, \n",
    "                  kernel_size=3,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\",\n",
    "                  input_shape=modelInputDim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 1. Hidden Layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Conv2D( filters=32, \n",
    "                  kernel_size=3, \n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 2. Output Layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1568)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                15690     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,258\n",
      "Trainable params: 25,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping( monitor=\"val_loss\", patience=10),\n",
    "    ModelCheckpoint(filepath=\"model_weight.h5\",\n",
    "                    monitor=\"val_accuracy\", verbose=1,\n",
    "                    save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.6076 - accuracy: 0.7788\n",
      "Epoch 1: val_accuracy improved from -inf to 0.84517, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 45s 29ms/step - loss: 0.6076 - accuracy: 0.7788 - val_loss: 0.4461 - val_accuracy: 0.8452\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8420\n",
      "Epoch 2: val_accuracy improved from 0.84517 to 0.87217, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 45s 30ms/step - loss: 0.4410 - accuracy: 0.8420 - val_loss: 0.3712 - val_accuracy: 0.8722\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.8606\n",
      "Epoch 3: val_accuracy improved from 0.87217 to 0.87983, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 41s 27ms/step - loss: 0.3948 - accuracy: 0.8606 - val_loss: 0.3453 - val_accuracy: 0.8798\n",
      "Epoch 4/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8704\n",
      "Epoch 4: val_accuracy improved from 0.87983 to 0.88392, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 41s 27ms/step - loss: 0.3647 - accuracy: 0.8704 - val_loss: 0.3192 - val_accuracy: 0.8839\n",
      "Epoch 5/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8777\n",
      "Epoch 5: val_accuracy improved from 0.88392 to 0.88900, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 43s 28ms/step - loss: 0.3422 - accuracy: 0.8777 - val_loss: 0.3171 - val_accuracy: 0.8890\n",
      "Epoch 6/10\n",
      "1498/1500 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8838\n",
      "Epoch 6: val_accuracy improved from 0.88900 to 0.89658, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 39s 26ms/step - loss: 0.3251 - accuracy: 0.8838 - val_loss: 0.2942 - val_accuracy: 0.8966\n",
      "Epoch 7/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.3103 - accuracy: 0.8876\n",
      "Epoch 7: val_accuracy improved from 0.89658 to 0.90450, saving model to model_weight.h5\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.3104 - accuracy: 0.8876 - val_loss: 0.2761 - val_accuracy: 0.9045\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8878\n",
      "Epoch 8: val_accuracy did not improve from 0.90450\n",
      "1500/1500 [==============================] - 41s 28ms/step - loss: 0.3065 - accuracy: 0.8878 - val_loss: 0.2838 - val_accuracy: 0.9000\n",
      "Epoch 9/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8931\n",
      "Epoch 9: val_accuracy did not improve from 0.90450\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.2955 - accuracy: 0.8931 - val_loss: 0.2962 - val_accuracy: 0.8938\n",
      "Epoch 10/10\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.8927\n",
      "Epoch 10: val_accuracy did not improve from 0.90450\n",
      "1500/1500 [==============================] - 39s 26ms/step - loss: 0.2888 - accuracy: 0.8927 - val_loss: 0.2709 - val_accuracy: 0.9035\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_trainClean,\n",
    "          y_trainOne,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.2846 - accuracy: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.284613698720932, 0.8980000019073486]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_testClean, y_testOne, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 이미지 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testImg = x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"../images/boots.jpg\", testImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "### 0. 이미지 로드\n",
    "testImgPath = \"../images/boots.jpg\"\n",
    "testImgReal = cv2.imread(testImgPath, cv2.IMREAD_GRAYSCALE)\n",
    "print(testImgReal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testImgRealClean = testImgReal.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testImgCleanComp = testImgRealClean.reshape(1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "predictValue = model.predict(testImgCleanComp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2269193e-07, 1.1539318e-08, 1.0892919e-07, 4.3500865e-08,\n",
       "        2.3492049e-07, 1.5195616e-04, 1.9122895e-06, 1.8537711e-02,\n",
       "        1.3222169e-05, 9.8129463e-01]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictValueMaxIndex = np.argmax( predictValue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answerLabel = fashion_mnist_labels[predictValueMaxIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testImgCleanComp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 이미지 전처리\n",
    "### 2. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageList = []\n",
    "# labelList = []\n",
    "# IMG_SIZE = 32\n",
    "\n",
    "# basedir = \"../images/mnist/trainingSet/\"\n",
    "\n",
    "# categories = os.listdir(basedir)\n",
    "\n",
    "# # 폴더 순환\n",
    "# for i in range (0, len(categories)):\n",
    "#     labelPath = os.path.join(basedir,categories[i])\n",
    "#     imagePath = os.listdir(labelPath)\n",
    "#     # 이미지 순환\n",
    "#     for j in range(0, len(imagePath)):\n",
    "#         imageFullPath = os.path.join(labelPath, imagePath[j])\n",
    "#         try:\n",
    "#             baseImg = cv2.imread(imageFullPath, cv2.IMREAD_GRAYSCALE)\n",
    "#             ### 1. 이미지 사이즈 변경\n",
    "#             refinedImg = cv2.resize(baseImg, (IMG_SIZE,IMG_SIZE))\n",
    "#             imageList.append(refinedImg)\n",
    "#             labelList.append(i)\n",
    "#         except Exception as e:\n",
    "#             print(e, imageFullPath)\n",
    "#             pass\n",
    "\n",
    "# imageArray = np.array(imageList)\n",
    "# labelArray = np.array(labelList)\n",
    "\n",
    "# print(imageArray.shape)\n",
    "# print(labelArray.shape)\n",
    "\n",
    "# imageDatas = [imageArray, labelArray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imageDatas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnistImage.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(imageDatas, f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imageDatas' is not defined"
     ]
    }
   ],
   "source": [
    "with open('mnistImage.pickle', 'wb') as f:\n",
    "    pickle.dump(imageDatas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnistImage.pickle', 'rb') as f:\n",
    "    imageDatas = pickle.load(f) # 단 한줄씩 읽어옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageArray = imageDatas[0]\n",
    "labelArray = imageDatas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_features,\\\n",
    "testData_features,\\\n",
    "trainingData_labels,\\\n",
    "testData_labels = \\\n",
    "train_test_split(imageArray, labelArray, test_size = 0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainingData_features.shape)\n",
    "print(testData_features.shape)\n",
    "print(trainingData_labels.shape)\n",
    "print(testData_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_features = trainingData_features.reshape(len(trainingData_features), \n",
    "                              IMG_SIZE, \n",
    "                              IMG_SIZE, \n",
    "                              1)\n",
    "testData_features = testData_features.reshape(len(testData_features), \n",
    "                          IMG_SIZE, \n",
    "                          IMG_SIZE, \n",
    "                          1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainingData_features.shape)\n",
    "print(testData_features.shape)\n",
    "print(trainingData_labels.shape)\n",
    "print(testData_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_features_norm = trainingData_features/255.0\n",
    "testData_features_norm = testData_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 접답지는 softmax랑 결합을 위한 onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_labels_one = to_categorical(trainingData_labels)\n",
    "testData_labels_one = to_categorical(testData_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(trainingData_labels_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = trainingData_features_norm[0].shape\n",
    "inputShape\n",
    "\n",
    "outputShape = len(trainingData_labels_one[0])\n",
    "outputShape\n",
    "\n",
    "## load vgg16 model 자중치 및 top input layer 제외\n",
    "vgg_model = VGG16(weights=None, include_top=False, input_shape=inputShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your own input format (here 3x200x200)\n",
    "inputs = Input(shape=inputShape,name = 'image_input')\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = vgg_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense레이어 추가\n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "# x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "# x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(outputShape, activation='softmax', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#커스텀 모델 생성\n",
    "my_model = Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a layer where input is the output of the  second last layer \n",
    "x = Dense(outputShape, activation='softmax', name='predictions')(vgg_model.layers[-2].output)\n",
    "\n",
    "#Then create the corresponding model \n",
    "my_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer= \"adam\",\n",
    "              metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "my_model.fit(x=trainingData_features_norm,\n",
    "              y=trainingData_labels_one,\n",
    "              epochs=epochs,\n",
    "              validation_data = (testData_features_norm, testData_labels_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델생성 시작\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=3, \n",
    "                 padding=\"same\", \n",
    "                 activation=\"relu\",\n",
    "                 input_shape=inputShape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size= 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(rate=0.4)) # 사용한 비율을 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=3, \n",
    "                 padding=\"same\", \n",
    "                 activation=\"relu\",\n",
    "                 input_shape=inputShape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size= 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=outputShape, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 컴파일 (loss, metrics, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "#               optimizer= keras.optimizers.Adam(),\n",
    "#               metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", \n",
    "              optimizer= \"adam\",\n",
    "              metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer= \"adam\",\n",
    "              metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 학습(훈련)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochNo = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainingData_features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33600 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=trainingData_features_norm, \n",
    "          y=trainingData_labels_one,\n",
    "          epochs=epochNo,\n",
    "          batch_size=32,\n",
    "          validation_data=(testData_features_norm, testData_labels_one) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x = testData_features_norm,\n",
    "               y = testData_labels_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refineImage(testImg):\n",
    "\n",
    "    # 사이즈 조정\n",
    "    IMG_SIZE=28  \n",
    "    # 컬러이미지 -> 회색톤으로 변경\n",
    "    img_gray = cv2.cvtColor(testImg, cv2.COLOR_RGB2GRAY)\n",
    "    img_resize = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
    "    ### shape 변경\n",
    "    img_shape = img_resize.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    ### 정규화\n",
    "    img_norm = img_shape/255.0\n",
    "    return img_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = cv2.imread(\"./sample_mnist.jpg\", cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refinedImageOne = refineImage(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictValue = np.argmax(model.predict(refinedImageOne))\n",
    "predictValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델구조저장\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"./model_mnist.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"./model_mnist_weight.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
